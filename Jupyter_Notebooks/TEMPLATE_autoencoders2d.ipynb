{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n",
      "Using TensorFlow backend.\n",
      "100%|██████████| 2/2 [00:00<00:00, 35.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling: \n",
      "1    5536\n",
      "0    5536\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "Train shape: \t7971, \n",
      "Test shape: \t1108, \n",
      "Valid shape: \t1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "seed = 2018\n",
    "np.random.seed(seed)\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "from utils import TEMP_DATADIR\n",
    "from wavhandler import *\n",
    "from sklearn.utils import shuffle\n",
    "from utils_train import *\n",
    "\n",
    "setting = 'gasf'\n",
    "model_name = 'autoencoder'\n",
    "undersampling = True\n",
    "dataset = Dataset('LG')\n",
    "dataset.load(only_names=True, nr_signals=np.inf)\n",
    "X_names = dataset.filenames\n",
    "y = dataset.y\n",
    "\n",
    "# Training setting - What kind of data to use\n",
    "if setting in ['gasf','gadf', 'mtf', 'rp']:\n",
    "    input_shape = (148,148,1)\n",
    "elif setting=='stft':\n",
    "    input_shape = (129, 120, 1)\n",
    "\n",
    "# More settings\n",
    "model_name = model_name + '_' + setting + '_'\n",
    "top_weights_path = TEMP_DATADIR + str(model_name) + '.h5'\n",
    "logfile = TEMP_DATADIR + str(model_name) + '.log'\n",
    "batch_size = 32\n",
    "monitor = 'val_loss'\n",
    "es_patience = 7\n",
    "rlr_patience = 3\n",
    "target_names = np.unique(y)\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "X = np.array(X_names).reshape(-1,1)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0) \n",
    "\n",
    "if undersampling:\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    ros = RandomUnderSampler(random_state=0)\n",
    "    ros.fit(X,y)\n",
    "    X, y = ros.fit_resample(X,y)\n",
    "    X = pd.Series(X.ravel()).tolist()\n",
    "    print('After undersampling: \\n{}\\n'.format(pd.DataFrame(y).iloc[:,0].value_counts()))\n",
    "else:\n",
    "    print('Class balance: \\n{}\\n'.format(pd.DataFrame(y).iloc[:,0].value_counts()))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.10, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                test_size=0.2, \n",
    "                                                stratify=y_train,  \n",
    "                                                random_state=0)\n",
    "print(\"Train shape: \\t{}, \\nTest shape: \\t{}, \\nValid shape: \\t{}\".format(len(X_train), len(X_test), len(X_val)))\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor = monitor,\n",
    "                                filepath = top_weights_path,\n",
    "                                save_best_only = True,\n",
    "                                save_weights_only = True,\n",
    "                                verbose = 1),\n",
    "                    EarlyStopping(monitor = monitor,\n",
    "                                patience = es_patience,\n",
    "                                verbose = 1),\n",
    "                    ReduceLROnPlateau(monitor = monitor,\n",
    "                                factor = 0.1,\n",
    "                                patience = rlr_patience,\n",
    "                                verbose = 1),\n",
    "                    CSVLogger(filename = logfile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (148, 148, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 74, 74, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 37, 1)         289       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 74, 74, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 74, 74, 32)        320       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 148, 148, 1)       577       \n",
      "=================================================================\n",
      "Total params: 38,786\n",
      "Trainable params: 38,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('input shape:', input_shape)\n",
    "# Encoder Layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, 3, activation='relu', padding='same', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(32, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(1, 3, activation='relu', padding='same'))\n",
    "model.add(UpSampling2D(2))\n",
    "model.add(Conv2D(32, 3, activation='relu', padding='same'))\n",
    "model.add(UpSampling2D(2))\n",
    "model.add(Conv2D(64, 3, activation='relu', padding='same'))\n",
    "model.add(Conv2D(1, 3, activation='relu', padding='same'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 70s 279ms/step - loss: 0.5902 - val_loss: 0.5847\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58465, saving model to /home/kalfasyan/projects/wingbeat_frequencies/temp_data/autoencoder_gasf_.h5\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 63s 250ms/step - loss: 0.5902 - val_loss: 0.5890\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.58465\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 62s 250ms/step - loss: 0.5902 - val_loss: 0.5912\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58465\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.5902 - val_loss: 0.5821\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58465 to 0.58207, saving model to /home/kalfasyan/projects/wingbeat_frequencies/temp_data/autoencoder_gasf_.h5\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.5902 - val_loss: 0.5922\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.58207\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 63s 251ms/step - loss: 0.5902 - val_loss: 0.5775\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58207 to 0.57750, saving model to /home/kalfasyan/projects/wingbeat_frequencies/temp_data/autoencoder_gasf_.h5\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.5902 - val_loss: 0.5956\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.57750\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 62s 246ms/step - loss: 0.5902 - val_loss: 0.5776\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.57750\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 63s 250ms/step - loss: 0.5902 - val_loss: 0.5933\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.57750\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 63s 254ms/step - loss: 0.5902 - val_loss: 0.5847\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.57750\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 64s 254ms/step - loss: 0.5902 - val_loss: 0.5890\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.57750\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.5902 - val_loss: 0.5912\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.57750\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.5902 - val_loss: 0.5821\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.57750\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1589ff2be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN\n",
    "model.fit_generator(train_generator2(X_train,\n",
    "                                    batch_size=batch_size, \n",
    "                                    target_names=target_names,\n",
    "                                    setting=setting),\n",
    "                    steps_per_epoch = int(math.ceil(float(len(X_train)) / float(batch_size))),\n",
    "                    epochs=100, \n",
    "                    validation_data = valid_generator2(X_val,\n",
    "                                                    batch_size=batch_size, \n",
    "                                                    target_names=target_names,\n",
    "                                                    setting=setting), \n",
    "                    validation_steps = int(math.ceil(float(len(X_test)) / float(batch_size))),\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a terminal and start TensorBoard to read logs in the autoencoder subdirectory\n",
    "# tensorboard --logdir=autoencoder\n",
    "\n",
    "# autoencoder.fit(x_train, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test, x_test),\n",
    "#                 callbacks=[TensorBoard(log_dir='conv_autoencoder')], verbose=2)\n",
    "\n",
    "# take a look at the reconstructed digits\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# take a look at the 128-dimensional encoded representation\n",
    "# these representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "\n",
    "# save latent space features 128-d vector\n",
    "pickle.dump(encoded_imgs, open('conv_autoe_features.pickle', 'wb'))\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(10, 4), dpi=100)\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
