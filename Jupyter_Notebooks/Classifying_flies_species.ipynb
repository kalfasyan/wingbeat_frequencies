{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wavhandler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Cichorii.\n",
      "Read 1451 filenames in 0.04 seconds.\n",
      "Loaded data into matrix in 6.12 seconds.\n",
      "Data: LG_drosophila_10_09.\n",
      "Read 5536 filenames in 0.08 seconds.\n",
      "Loaded data into matrix in 21.73 seconds.\n",
      "Data: LG_zapr_26_09.\n",
      "Read 7210 filenames in 0.08 seconds.\n",
      "Loaded data into matrix in 28.13 seconds.\n",
      "Data: D. suzukii.\n",
      "Read 2401 filenames in 0.02 seconds.\n",
      "Loaded data into matrix in 9.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "data1 = Dataset('Leafminers')\n",
    "data1.read(data=data1.target_classes[0], setting='psd_dB', labels='text')\n",
    "\n",
    "data2 = Dataset('LG')\n",
    "data2.read(data=data2.target_classes[0], setting='psd_dB', labels='text')\n",
    "\n",
    "data3 = Dataset('LG')\n",
    "data3.read(data=data3.target_classes[1], setting='psd_dB', labels='text')\n",
    "\n",
    "data4 = Dataset('Pcfruit')\n",
    "data4.read(data=data4.target_classes[1], setting='psd_dB', labels='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.clean(plot=False)\n",
    "data2.clean(plot=False)\n",
    "data3.clean(plot=False)\n",
    "data4.clean(plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15405, 130)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big = pd.DataFrame()\n",
    "\n",
    "big = pd.concat([data1.X, data2.X, data3.X, data4.X], axis=0)\n",
    "big['y'] = pd.concat([data1.y, data2.y, data3.y, data4.y], axis=0)\n",
    "big.dropna(how='any', axis=1, inplace=True)\n",
    "big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LG_zapr_26_09          7002\n",
       "LG_drosophila_10_09    5277\n",
       "D. suzukii             2373\n",
       "Cichorii                753\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance: \n",
      "3    7210\n",
      "2    5536\n",
      "1    2401\n",
      "0    1451\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "Epoch 1/100\n",
      "374/374 [==============================] - 83s 221ms/step - loss: 0.1806 - acc: 0.9321 - val_loss: 1.4274 - val_acc: 0.7091\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70913, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test__stft_wrapper.h5\n",
      "Epoch 2/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0972 - acc: 0.9620 - val_loss: 1.2891 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70913 to 0.76095, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test__stft_wrapper.h5\n",
      "Epoch 3/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0751 - acc: 0.9723 - val_loss: 1.3968 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76095 to 0.77344, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test__stft_wrapper.h5\n",
      "Epoch 4/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0610 - acc: 0.9766 - val_loss: 1.3755 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77344\n",
      "Epoch 5/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0532 - acc: 0.9792 - val_loss: 0.3036 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77344 to 0.88882, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test__stft_wrapper.h5\n",
      "Epoch 6/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0505 - acc: 0.9812 - val_loss: 0.4056 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88882\n",
      "Epoch 7/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0473 - acc: 0.9819 - val_loss: 2.2383 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.88882\n",
      "Epoch 8/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0381 - acc: 0.9849 - val_loss: 4.7236 - val_acc: 0.3887\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.88882\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0243 - acc: 0.9898 - val_loss: 0.0225 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.88882 to 0.99279, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test__stft_wrapper.h5\n",
      "Epoch 10/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0155 - acc: 0.9938 - val_loss: 0.0480 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.99279\n",
      "Epoch 11/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0118 - acc: 0.9955 - val_loss: 0.0476 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.99279\n",
      "Epoch 12/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 0.0893 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.99279\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/100\n",
      "374/374 [==============================] - 59s 158ms/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.0307 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.99279\n",
      "Epoch 14/100\n",
      "374/374 [==============================] - 59s 157ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0379 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.99279\n",
      "Epoch 15/100\n",
      "374/374 [==============================] - 58s 156ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.99279\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 16/100\n",
      "374/374 [==============================] - 58s 156ms/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0304 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.99279\n",
      "Epoch 00016: early stopping\n",
      "loss 0.024410555831504515\n",
      "Test accuracy: 0.9903614457831326\n"
     ]
    }
   ],
   "source": [
    "from utils_train import make_classification_conv1d, make_classification_conv2d\n",
    "\n",
    "big_names = pd.DataFrame(pd.concat([data1.filenames, data2.filenames, \n",
    "                                    data3.filenames, data4.filenames], axis=0), columns=['filenames']).reset_index(drop=True)\n",
    "big_names['y'] = big_names.filenames.apply(lambda x: x.split('/')[6])\n",
    "#pd.concat([data1.y, data2.y, data3.y, data4.y], axis=0).reset_index(drop=True)\n",
    "\n",
    "# big_names.dropna(how='any', axis=0, inplace=True)\n",
    "big_names.sample(5)\n",
    "big_names.y.value_counts()\n",
    "make_classification_conv2d(big_names.filenames.tolist(), big_names.y, undersampling=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trained 2D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from utils_train import *\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "path = './temp_data/'\n",
    "yaml_file = open(path + 'test__stft_wrapper.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path + \"test__stft_wrapper.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "yy = le.fit_transform(big_names.y)\n",
    "\n",
    "X = big_names.iloc[:,:-1]\n",
    "y = yy\n",
    "targets = len(np.unique(y))\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104/104 [==============================] - 4s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.023080089985949116, 0.9891566265060241]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "loaded_model.evaluate_generator(valid_generator(X_test.filenames.tolist(), \n",
    "                                                y_test.tolist(), \n",
    "                                                batch_size=batch_size, \n",
    "                                                setting='stft', \n",
    "                                                target_names=np.unique(y_test).tolist()),\n",
    "                                steps = int(math.ceil(float(len(X_test)) / float(batch_size))),\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 1D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11091 samples, validate on 2773 samples\n",
      "Epoch 1/100\n",
      "11091/11091 [==============================] - 8s 707us/step - loss: 0.2838 - acc: 0.8931 - val_loss: 13.6324 - val_acc: 0.1536\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.15362, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 2/100\n",
      "11091/11091 [==============================] - 4s 318us/step - loss: 0.1871 - acc: 0.9346 - val_loss: 4.5148 - val_acc: 0.2575\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.15362 to 0.25748, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 3/100\n",
      "11091/11091 [==============================] - 4s 336us/step - loss: 0.1649 - acc: 0.9372 - val_loss: 12.7893 - val_acc: 0.1583\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.25748\n",
      "Epoch 4/100\n",
      "11091/11091 [==============================] - 4s 322us/step - loss: 0.1473 - acc: 0.9466 - val_loss: 0.8259 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.25748 to 0.72881, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 5/100\n",
      "11091/11091 [==============================] - 4s 334us/step - loss: 0.1262 - acc: 0.9535 - val_loss: 7.2212 - val_acc: 0.2153\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.72881\n",
      "Epoch 6/100\n",
      "11091/11091 [==============================] - 4s 338us/step - loss: 0.1203 - acc: 0.9580 - val_loss: 7.6794 - val_acc: 0.1828\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72881\n",
      "Epoch 7/100\n",
      "11091/11091 [==============================] - 4s 330us/step - loss: 0.1019 - acc: 0.9633 - val_loss: 0.5023 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72881 to 0.80454, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 8/100\n",
      "11091/11091 [==============================] - 4s 335us/step - loss: 0.1030 - acc: 0.9604 - val_loss: 0.3485 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80454 to 0.85683, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 9/100\n",
      "11091/11091 [==============================] - 4s 337us/step - loss: 0.0970 - acc: 0.9650 - val_loss: 6.0508 - val_acc: 0.2441\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85683\n",
      "Epoch 10/100\n",
      "11091/11091 [==============================] - 4s 339us/step - loss: 0.0946 - acc: 0.9658 - val_loss: 0.2513 - val_acc: 0.9135\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85683 to 0.91345, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 11/100\n",
      "11091/11091 [==============================] - 4s 338us/step - loss: 0.0862 - acc: 0.9702 - val_loss: 13.3104 - val_acc: 0.1540\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91345\n",
      "Epoch 12/100\n",
      "11091/11091 [==============================] - 4s 371us/step - loss: 0.0877 - acc: 0.9675 - val_loss: 1.2648 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91345\n",
      "Epoch 13/100\n",
      "11091/11091 [==============================] - 4s 350us/step - loss: 0.0843 - acc: 0.9689 - val_loss: 2.2938 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91345\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/100\n",
      "11091/11091 [==============================] - 4s 349us/step - loss: 0.0633 - acc: 0.9773 - val_loss: 0.1770 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.91345 to 0.93942, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 15/100\n",
      "11091/11091 [==============================] - 4s 345us/step - loss: 0.0457 - acc: 0.9840 - val_loss: 0.3872 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93942\n",
      "Epoch 16/100\n",
      "11091/11091 [==============================] - 4s 345us/step - loss: 0.0402 - acc: 0.9860 - val_loss: 0.0981 - val_acc: 0.9647\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.93942 to 0.96466, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 17/100\n",
      "11091/11091 [==============================] - 4s 347us/step - loss: 0.0382 - acc: 0.9852 - val_loss: 0.0821 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.96466 to 0.97223, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 18/100\n",
      "11091/11091 [==============================] - 4s 343us/step - loss: 0.0355 - acc: 0.9872 - val_loss: 0.1317 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.97223\n",
      "Epoch 19/100\n",
      "11091/11091 [==============================] - 4s 342us/step - loss: 0.0346 - acc: 0.9883 - val_loss: 0.0783 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97223 to 0.97295, saving model to /media/kalfasyan/HGST_4TB/Ubudirs/projects/wingbeat_frequencies/temp_data/test_.h5\n",
      "Epoch 20/100\n",
      "11091/11091 [==============================] - 4s 354us/step - loss: 0.0328 - acc: 0.9883 - val_loss: 0.1035 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.97295\n",
      "Epoch 21/100\n",
      "11091/11091 [==============================] - 4s 336us/step - loss: 0.0266 - acc: 0.9920 - val_loss: 0.0943 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.97295\n",
      "Epoch 22/100\n",
      "11091/11091 [==============================] - 4s 353us/step - loss: 0.0271 - acc: 0.9915 - val_loss: 0.2195 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.97295\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 23/100\n",
      "11091/11091 [==============================] - 4s 350us/step - loss: 0.0240 - acc: 0.9920 - val_loss: 0.0790 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.97295\n",
      "Epoch 24/100\n",
      "11091/11091 [==============================] - 4s 367us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0779 - val_acc: 0.9726\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.97295\n",
      "Epoch 25/100\n",
      "11091/11091 [==============================] - 4s 354us/step - loss: 0.0231 - acc: 0.9929 - val_loss: 0.0785 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.97295\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00025: early stopping\n",
      "1541/1541 [==============================] - 0s 224us/step\n",
      "loss 0.06758605706378411\n",
      "Test accuracy: 0.972744970798183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yy = le.fit_transform(big.y)\n",
    "\n",
    "make_classification_conv1d(big.iloc[:,:-1], yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trained 1D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "path = './temp_data/'\n",
    "yaml_file = open(path + 'test__raw_final.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(path + \"test_.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "#loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541/1541 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06758605744891409, 0.972744970798183]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_train import *\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X = big.iloc[:,:-1]\n",
    "y = yy\n",
    "targets = len(np.unique(y))\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = train_test_val_split(X,y, random_state=0)\n",
    "\n",
    "# Convert label to onehot\n",
    "y_train = to_categorical(y_train, num_classes=targets)\n",
    "y_val = to_categorical(y_val, num_classes=targets)\n",
    "y_test = to_categorical(y_test, num_classes=targets)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "\n",
    "loaded_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.get_sensor_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.df_features.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.get_sensor_features(version='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.df_features.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = Dataset('Leafminers')\n",
    "dt1.read(data=dt1.target_classes[0], setting='read', labels='text')\n",
    "dt1.get_frequency_peaks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = Dataset('Leafminers')\n",
    "dt2.read(data=dt1.target_classes[1], setting='read', labels='text')\n",
    "dt2.get_frequency_peaks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_analysis(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.get_sensor_features(version='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_inds(dataset, th):\n",
    "    df = dataset.X.copy()\n",
    "    dataset.y.index = list(dataset.y.reset_index(drop=True).index)\n",
    "    df['var'] = df.apply(lambda x: x.iloc[10:50].var(), axis=1)\n",
    "    inds = df[(df['var']>th)].index\n",
    "    return inds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_hist(data2.df_features.loc[clean_inds(data2, 10)].dropna(), 'temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,8))\n",
    "np_hist(data2.df_features, 'date_hour', rot=0, fs=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "np_hist(data2.df_features, 'humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.df_features.date.sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
