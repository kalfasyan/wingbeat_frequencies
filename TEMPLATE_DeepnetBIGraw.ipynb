{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wavhandler import *\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 37.39it/s]\n"
     ]
    }
   ],
   "source": [
    "data1 = Dataset('Wingbeats')\n",
    "data1.load(only_names=True, nr_signals=5000, text_labels=True);\n",
    "data2 = Dataset('LG')\n",
    "data2.load(only_names=True, text_labels=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X_names = data1.filenames + data2.filenames\n",
    "data_y = data1.y + data2.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.66 s, sys: 2.95 s, total: 10.6 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = make_df_parallel(names=data_X_names, setting='read').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: \t(32580, 5000), \n",
      "Test shape: \t(4526, 5000), \n",
      "Valid shape: \t(8145, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = LabelEncoder().fit_transform(y=data_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.values, y, \n",
    "                                                    test_size=0.10, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=0)\n",
    "print(\"Train shape: \\t{}, \\nTest shape: \\t{}, \\nValid shape: \\t{}\".format(X_train.shape, X_test.shape, X_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nr_classes = len(np.unique(data_y))\n",
    "\n",
    "# Convert label to onehot\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=nr_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=nr_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=nr_classes)\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_val = np.expand_dims(X_val, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "# Build the Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16, 3, activation='relu', input_shape=(5000, 1)))\n",
    "model.add(Conv1D(16, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nr_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'raw_BIG'\n",
    "top_weights_path = TEMP_DATADIR + '/model_' + str(model_name) + '.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [ModelCheckpoint(top_weights_path, monitor = 'val_acc', verbose = 1, save_best_only = True, save_weights_only = True),\n",
    "    EarlyStopping(monitor = 'val_acc', patience = 6, verbose = 1),\n",
    "    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 3, verbose = 1),\n",
    "    CSVLogger('model_' + str(model_name) + '.log')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data = [X_val, y_val], callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275/1275 [==============================] - 4s 3ms/step\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(top_weights_path)\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "#print('loss', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(TEMP_DATADIR + model_name + \".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "model.save_weights(TEMP_DATADIR + model_name + \"_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
