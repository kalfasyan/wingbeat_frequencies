{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed = 2018\n",
    "np.random.seed(seed)\n",
    "from wavhandler import *\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import math\n",
    "import warnings\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = DenseNet121\n",
    "\n",
    "model_name = 'my_Wingbeats' + current_model.__name__\n",
    "top_weights_path = TEMP_DATADIR + 'model_' + str(model_name) + '.h5'\n",
    "logfile = TEMP_DATADIR + 'model_' + str(model_name) + '.log'\n",
    "batch_size = 32\n",
    "monitor = 'val_acc'\n",
    "input_shape = (129, 120, 1)\n",
    "es_patience = 7\n",
    "rlr_patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# data1 = Dataset('increasing dataset')\n",
    "# data1.target_classes = [i for i in data1.target_classes if \"aedes\" not in i.split('_')]\n",
    "# data1.load(only_names=True, text_labels=True)\n",
    "data2 = Dataset('Wingbeats')\n",
    "data2.load(only_names=True, nr_signals=np.inf, text_labels=True);\n",
    "# data3 = Dataset('LG')\n",
    "# data3.load(only_names=True, text_labels=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of all classes: \n",
      "['Ae. aegypti' 'Ae. albopictus' 'An. arabiensis' 'An. gambiae'\n",
      " 'C. pipiens' 'C. quinquefasciatus']\n"
     ]
    }
   ],
   "source": [
    "X_names = data2.filenames #+ data1.filenames #+ data3.filenames\n",
    "y = data2.y #+ data1.y #+ data3.y\n",
    "target_names = np.unique(y)\n",
    "print(\"Names of all classes: \\n{}\".format(target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Val generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(X_train, y_train, batch_size, target_names):\n",
    "    while True:\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            end = min(start + batch_size, len(X_train))\n",
    "            train_batch = X_train[start:end]\n",
    "            labels_batch = y_train[start:end]\n",
    "            \n",
    "            for i in range(len(train_batch)):\n",
    "                data, rate = librosa.load(train_batch[i], sr = SR)\n",
    "                if 'increasing dataset' in train_batch[i].split('/'):\n",
    "                    data = crop_rec(data)\n",
    "\n",
    "#                 data = random_data_shift(data, u = .2)\n",
    "\n",
    "                data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    data = librosa.amplitude_to_db(data)\n",
    "\n",
    "                data = np.flipud(data)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "                y_batch.append(labels_batch[i])\n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "            \n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "\n",
    "def valid_generator(X_val, y_val, batch_size, target_names):\n",
    "    while True:\n",
    "        for start in range(0, len(X_val), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            end = min(start + batch_size, len(X_val))\n",
    "            test_batch = X_val[start:end]\n",
    "            labels_batch = y_val[start:end]\n",
    "\n",
    "            for i in range(len(test_batch)):\n",
    "                data, rate = librosa.load(test_batch[i], sr = SR)\n",
    "                if 'increasing dataset' in test_batch[i].split('/'):\n",
    "                    data = crop_rec(data)\n",
    "\n",
    "                data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    data = librosa.amplitude_to_db(data)\n",
    "                data = np.flipud(data)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "                y_batch.append(labels_batch[i])\n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "\n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: \t201287, \n",
      "Test shape: \t27957, \n",
      "Valid shape: \t50322\n"
     ]
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_names, y, \n",
    "                                                    test_size=0.10, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=0)\n",
    "print(\"Train shape: \\t{}, \\nTest shape: \\t{}, \\nValid shape: \\t{}\".format(len(X_train), len(X_test), len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [ModelCheckpoint(top_weights_path, monitor = 'val_acc', verbose = 1, save_best_only = True, save_weights_only = True),\n",
    "    EarlyStopping(monitor = 'val_acc', patience = 6, verbose = 1),\n",
    "    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 3, verbose = 1),\n",
    "    CSVLogger(logfile)]\n",
    "\n",
    "img_input = Input(shape = input_shape)\n",
    "\n",
    "model = current_model(input_tensor = img_input, classes = len(target_names), weights = None)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor = monitor,\n",
    "                                filepath = top_weights_path,\n",
    "                                save_best_only = True,\n",
    "                                save_weights_only = True,\n",
    "                                verbose = 1),\n",
    "                    EarlyStopping(monitor = monitor,\n",
    "                                patience = es_patience,\n",
    "                                verbose = 1),\n",
    "                    ReduceLROnPlateau(monitor = monitor,\n",
    "                                factor = 0.1,\n",
    "                                patience = rlr_patience,\n",
    "                                verbose = 1),\n",
    "                    CSVLogger(filename = logfile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 160/6291 [..............................] - ETA: 1:00:28 - loss: 0.9414 - acc: 0.6594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3291, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-f070a00c842d>\", line 12, in <module>\n",
      "    callbacks = callbacks_list)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 217, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/engine/training.py\", line 1217, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/kalfasyan/miniconda3/envs/wingbeat2/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator(X_train,\n",
    "                                    y_train, \n",
    "                                    batch_size=32, \n",
    "                                    target_names=target_names),\n",
    "                    steps_per_epoch = int(math.ceil(float(len(X_train)) / float(batch_size))),\n",
    "                    epochs=100, \n",
    "                    validation_data = valid_generator(X_val,\n",
    "                                                      y_val, \n",
    "                                                      batch_size=32, \n",
    "                                                      target_names=target_names), \n",
    "                    validation_steps = int(math.ceil(float(len(X_test)) / float(batch_size))),\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(top_weights_path)\n",
    "\n",
    "loss, acc = model.evaluate_generator(valid_generator(X_val, y_val, batch_size=32),\n",
    "        steps = int(math.ceil(float(len(X_test)) / float(batch_size))))\n",
    "#print('loss', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(TEMP_DATADIR + model_name + \".yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "model.save_weights(TEMP_DATADIR + model_name + \"_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
