{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed = 2018\n",
    "np.random.seed(seed)\n",
    "from wavhandler import *\n",
    "from utils import *\n",
    "from utils_train import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_model = DenseNet121\n",
    "# current_model = DenseNet169\n",
    "# current_model = DenseNet201\n",
    "# current_model = InceptionResNetV2\n",
    "# current_model = InceptionV3\n",
    "# current_model = MobileNet\n",
    "# current_model = NASNetLarge\n",
    "# current_model = NASNetMobile\n",
    "# current_model = VGG16\n",
    "# current_model = VGG19\n",
    "current_model = Xception\n",
    "\n",
    "model_name = TEMP_DATADIR + 'wingbeats_' + current_model.__name__\n",
    "\n",
    "best_weights_path = model_name + '.h5'\n",
    "log_path = model_name + '.log'\n",
    "monitor = 'val_acc'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "es_patience = 7\n",
    "rlr_patience = 3\n",
    "\n",
    "SR = 8000\n",
    "N_FFT = 256\n",
    "HOP_LEN = int(N_FFT / 6)\n",
    "input_shape = (129, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ae. aegypti', 'Ae. albopictus', 'An. gambiae', 'An. arabiensis', 'C. pipiens', 'C. quinquefasciatus']\n"
     ]
    }
   ],
   "source": [
    "target_names = mosquitos_6#os.listdir(DATADIR)\n",
    "\n",
    "filenames, y = get_data(#filedir= DATADIR,\n",
    "                      target_names=target_names, nr_signals=np.inf, only_names=True)\n",
    "print(target_names)\n",
    "\n",
    "X_names, y = shuffle(filenames, y, random_state = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_names, y, stratify = y, test_size = 0.20, random_state = seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.31 s ± 42.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z = pd.Series(X_test).apply(lambda x: get_wingbeat_timestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.33 s ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "zz = list(map(get_wingbeat_timestamp, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Input(shape = input_shape)\n",
    "\n",
    "model = current_model(input_tensor = img_input, classes = len(target_names), weights = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "    # define our MLP network\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model1.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model1.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # return our model\n",
    "    return model1\n",
    "\n",
    "mlp = create_mlp(24, regress=False)\n",
    "combinedInput = concatenate([model.output, mlp.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Dense(6, activation='relu')(combinedInput)\n",
    "z = Dense(6, activation='softmax')(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = Model(inputs=[mlp.input, model.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.compile(optimizer = 'adam', \n",
    "                       loss = 'categorical_crossentropy', \n",
    "                       metrics = ['accuracy'])\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor = monitor,\n",
    "                                filepath = best_weights_path,\n",
    "                                save_best_only = True,\n",
    "                                save_weights_only = True,\n",
    "                                verbose = 1),\n",
    "                    EarlyStopping(monitor = monitor,\n",
    "                                patience = es_patience,\n",
    "                                verbose = 1),\n",
    "                    ReduceLROnPlateau(monitor = monitor,\n",
    "                                factor = 0.1,\n",
    "                                patience = rlr_patience,\n",
    "                                verbose = 1),\n",
    "                    CSVLogger(filename = log_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(X_train, X2_train, y_train, batch_size, target_names, crop=False):\n",
    "    while True:\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            x_batch = []\n",
    "#             x2_batch = []\n",
    "            y_batch = []\n",
    "#             y2_batch = []\n",
    "\n",
    "            end = min(start + batch_size, len(X_train))\n",
    "            train_batch = X_train[start:end]\n",
    "            labels_batch = y_train[start:end]\n",
    "\n",
    "            for i in range(len(train_batch)):\n",
    "                data, rate = librosa.load(train_batch[i], sr = SR)\n",
    "                if crop:\n",
    "                    data = crop_rec(data)\n",
    "\n",
    "                data = random_data_shift(data, u = .2)\n",
    "\n",
    "                data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                data = librosa.amplitude_to_db(np.abs(data))\n",
    "                data = np.flipud(data)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "                data = random_data_shift(data, w_limit = (-0.25, 0.25), h_limit = (-0.0, 0.0), cval = np.min(data), u = 1.0)\n",
    "\n",
    "                # data = np.squeeze(data, axis = -1)\n",
    "                # plt.imshow(data, cmap = 'gray')\n",
    "                # plt.show()\n",
    "                # data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "#                 x2_batch.append(get_wingbeat_timestamp(train_batch[i]))\n",
    "                y_batch.append(labels_batch[i])\n",
    "#                 y2_batch = \n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "#             x2_batch = np.array(x2_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "#             y2_batch = np.array(y2_batch, np.float32)\n",
    "\n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "\n",
    "            yield [x_batch, x2_batch, y_batch]\n",
    "\n",
    "def valid_generator(X_test, y_test, batch_size, target_names, crop=False):\n",
    "    while True:\n",
    "        for start in range(0, len(X_test), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            end = min(start + batch_size, len(X_test))\n",
    "            test_batch = X_test[start:end]\n",
    "            labels_batch = y_test[start:end]\n",
    "\n",
    "            for i in range(len(test_batch)):\n",
    "                data, rate = librosa.load(test_batch[i], sr = SR)\n",
    "                if crop:\n",
    "                    data = crop_rec(data)\n",
    "\n",
    "                data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                data = librosa.amplitude_to_db(np.abs(data))\n",
    "                data = np.flipud(data)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "                y_batch.append(labels_batch[i])\n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "\n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "crop = False\n",
    "x_batch = []\n",
    "#             x2_batch = []\n",
    "y_batch = []\n",
    "#             y2_batch = []\n",
    "\n",
    "end = min(start + batch_size, len(X_train))\n",
    "train_batch = X_train[start:end]\n",
    "labels_batch = y_train[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    data, rate = librosa.load(train_batch[i], sr = SR)\n",
    "    if crop:\n",
    "        data = crop_rec(data)\n",
    "\n",
    "#     data = random_data_shift(data, u = .2)\n",
    "\n",
    "    data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "    data = librosa.amplitude_to_db(np.abs(data))\n",
    "    data = np.flipud(data)\n",
    "\n",
    "    data = np.expand_dims(data, axis = -1)\n",
    "    data = random_data_shift(data, w_limit = (-0.25, 0.25), h_limit = (-0.0, 0.0), cval = np.min(data), u = 1.0)\n",
    "\n",
    "    # data = np.squeeze(data, axis = -1)\n",
    "    # plt.imshow(data, cmap = 'gray')\n",
    "    # plt.show()\n",
    "    # data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "    x_batch.append(data)\n",
    "#                 x2_batch.append(get_wingbeat_timestamp(train_batch[i]))\n",
    "    y_batch.append(labels_batch[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = np.array(x_batch, np.float32)\n",
    "#             x2_batch = np.array(x2_batch, np.float32)\n",
    "y_batch = np.array(y_batch, np.float32)\n",
    "#             y2_batch = np.array(y2_batch, np.float32)\n",
    "\n",
    "y_batch = np_utils.to_categorical(y_batch, len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 6)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "5\n",
      "21\n",
      "10\n",
      "14\n",
      "16\n",
      "14\n",
      "9\n",
      "0\n",
      "19\n",
      "12\n",
      "17\n",
      "19\n",
      "11\n",
      "21\n",
      "6\n",
      "14\n",
      "16\n",
      "22\n",
      "13\n",
      "15\n",
      "14\n",
      "20\n",
      "15\n",
      "16\n",
      "15\n",
      "14\n",
      "22\n",
      "16\n",
      "0\n",
      "17\n",
      "14\n",
      "19\n",
      "3\n",
      "4\n",
      "12\n",
      "17\n",
      "4\n",
      "22\n",
      "13\n",
      "11\n",
      "11\n",
      "19\n",
      "15\n",
      "11\n",
      "20\n",
      "19\n",
      "12\n",
      "12\n",
      "20\n",
      "13\n",
      "23\n",
      "19\n",
      "11\n",
      "17\n",
      "12\n",
      "9\n",
      "10\n",
      "2\n",
      "14\n",
      "13\n",
      "17\n",
      "17\n",
      "18\n",
      "23\n",
      "13\n",
      "16\n",
      "15\n",
      "10\n",
      "9\n",
      "0\n",
      "11\n",
      "22\n",
      "4\n",
      "23\n",
      "14\n",
      "6\n",
      "20\n",
      "15\n",
      "5\n",
      "18\n",
      "19\n",
      "0\n",
      "20\n",
      "19\n",
      "21\n",
      "17\n",
      "0\n",
      "19\n",
      "20\n",
      "14\n",
      "11\n",
      "13\n",
      "17\n",
      "14\n",
      "16\n",
      "22\n",
      "1\n",
      "22\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(X_train[:100]):\n",
    "#     print(i,x)\n",
    "    print(get_wingbeat_timestamp(x).hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
