{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2D N_FFT 256 - 25 EPOCHS - 0.945 ACC FULL SET // 20% TEST\n",
    "from wavhandler import *\n",
    "from utils import *\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "seed = 2018\n",
    "np.random.seed(seed)\n",
    "\n",
    "import librosa\n",
    "from scipy import signal\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalAveragePooling2D\n",
    "from keras.layers import Permute\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_gru'\n",
    "best_weights_path = TEMP_DATADIR + model_name + '.h5'\n",
    "log_path = TEMP_DATADIR + model_name + '.log'\n",
    "monitor = 'val_acc'\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "es_patience = 7\n",
    "rlr_patience = 3\n",
    "\n",
    "#model_mode, input_shape_1D = '1D_psd', (129, 1)\n",
    "#model_mode, input_shape_1D = '1D', (5000, 1)\n",
    "#model_mode, N_FFT, HOP_LEN, input_shape_2D = '2D', 128, 128, (65, 40, 1)\n",
    "model_mode, N_FFT, HOP_LEN, input_shape_2D = '2D', 256, int(256 / 6), (129, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train #recs =  96724\n",
      "test #recs =  24182\n"
     ]
    }
   ],
   "source": [
    "target_names = mosquitos_6\n",
    "\n",
    "DATADIR = '/home/kalfasyan/data/insects/Wingbeats/'\n",
    "# DATADIR = '/data/leuven/314/vsc31431/insects/Wingbeats/'\n",
    "X_names, y = get_data(filedir=DATADIR, target_names=target_names, only_names=True)\n",
    "\n",
    "X_names, y = shuffle(X_names, y, random_state = seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_names, y, stratify = y, test_size = 0.20, random_state = seed)\n",
    "\n",
    "print ('train #recs = ', len(X_train))\n",
    "print ('test #recs = ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cnn_gru_1D(cols, channels, num_classes):\n",
    "    inputs = Input(shape = (cols, channels))\n",
    "\n",
    "    x = BatchNormalization() (inputs)\n",
    "    x = Conv1D(16, kernel_size = 3, padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling1D((2)) (x)\n",
    "    x = Conv1D(32, kernel_size = 3, padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling1D((2)) (x)\n",
    "    x = Conv1D(64, kernel_size = 3, padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling1D((2)) (x)\n",
    "    x = Conv1D(128, kernel_size = 3, padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling1D((2)) (x)\n",
    "    x = Conv1D(256, kernel_size = 3, padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling1D((2)) (x)\n",
    "\n",
    "    #x = GlobalAveragePooling1D() (x)\n",
    "\n",
    "    x = CuDNNGRU(units = 128, return_sequences = True) (x)\n",
    "    x = CuDNNGRU(units = 128, return_sequences = False) (x)\n",
    "    #x = CuDNNLSTM(units = 128, return_sequences = True) (x)\n",
    "    #x = CuDNNLSTM(units = 128, return_sequences = False) (x)\n",
    "\n",
    "    x = Dropout(0.5) (x)\n",
    "\n",
    "    x = Dense(num_classes) (x)\n",
    "    outputs = Activation('softmax') (x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def basic_cnn_gru_2D(rows, cols, channels, num_classes):\n",
    "    inputs = Input(shape = (rows, cols, channels))\n",
    "\n",
    "    x = BatchNormalization() (inputs)\n",
    "    x = Conv2D(16, kernel_size = (3, 3), padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling2D((2,2)) (x)\n",
    "    x = Conv2D(32, kernel_size = (3, 3), padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling2D((2,2)) (x)\n",
    "    x = Conv2D(64, kernel_size = (3, 3), padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling2D((2,2)) (x)\n",
    "    x = Conv2D(128, kernel_size = (3, 3), padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling2D((2,2)) (x)\n",
    "    x = Conv2D(256, kernel_size = (3, 3), padding = 'same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Activation('elu') (x)\n",
    "    x = MaxPooling2D((2,2)) (x)\n",
    "    \n",
    "    #x = GlobalAveragePooling2D() (x)\n",
    "\n",
    "    x = Permute((2, 1, 3)) (x)\n",
    "\n",
    "    if N_FFT == 128:\n",
    "        x = Reshape((1, 2 * 256)) (x)\n",
    "    elif N_FFT == 256:\n",
    "        x = Reshape((3, 4 * 256)) (x)\n",
    "\n",
    "    x = CuDNNGRU(units = 128, return_sequences = True) (x)\n",
    "    x = CuDNNGRU(units = 128, return_sequences = False) (x)\n",
    "    #x = CuDNNLSTM(units = 128, return_sequences = True) (x)\n",
    "    #x = CuDNNLSTM(units = 128, return_sequences = False) (x)\n",
    "\n",
    "    x = Dropout(0.5) (x)\n",
    "\n",
    "    x = Dense(num_classes) (x)\n",
    "    outputs = Activation('softmax') (x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(x, wshift, hshift, row_axis = 0, col_axis = 1, channel_axis = 2, fill_mode = 'constant', cval = 0.):\n",
    "    h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "    tx = hshift * h\n",
    "    ty = wshift * w\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                   [0, 1, ty],\n",
    "                                   [0, 0, 1]])\n",
    "    transform_matrix = translation_matrix\n",
    "    x = image.apply_transform(x, transform_matrix, channel_axis, fill_mode, cval)\n",
    "    return x\n",
    "\n",
    "def random_data_shift(data, w_limit = (-0.25, 0.25), h_limit = (-0.0, 0.0), cval = 0., u = 0.5):\n",
    "    if np.random.random() < u:\n",
    "        wshift = np.random.uniform(w_limit[0], w_limit[1])\n",
    "        hshift = np.random.uniform(h_limit[0], h_limit[1])\n",
    "        data = shift(data, wshift, hshift, cval = cval)\n",
    "    return data\n",
    "\n",
    "# def random_data_shift(data, u = 0.5):\n",
    "#     if np.random.random() < u:\n",
    "#         data = np.roll(data, int(round(np.random.uniform(-(len(data)), (len(data))))))\n",
    "#     return data\n",
    "\n",
    "def train_generator(mode = '2D'):\n",
    "    while True:\n",
    "        for start in range(0, len(X_train), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            end = min(start + batch_size, len(X_train))\n",
    "            train_batch = X_train[start:end]\n",
    "            labels_batch = y_train[start:end]\n",
    "            \n",
    "            for i in range(len(train_batch)):\n",
    "                data, rate = librosa.load(train_batch[i], sr = SR)\n",
    "\n",
    "                #data = random_data_shift(data, u = 1.0)\n",
    "\n",
    "                if mode == '1D_psd':\n",
    "                    XX = np.zeros(129).astype('float32')\n",
    "                    XX = 10 * np.log10(signal.welch(data, fs = SR, window = 'hanning', nperseg = 256, noverlap = 128 + 64)[1])\n",
    "                    data = XX\n",
    "\n",
    "                if mode == '2D':\n",
    "                    data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                    data = librosa.amplitude_to_db(np.abs(data))\n",
    "\n",
    "                    data = np.flipud(data)\n",
    "\n",
    "                    data = np.expand_dims(data, axis = -1)\n",
    "                    data = random_data_shift(data, w_limit = (-0.25, 0.25), h_limit = (-0.0, 0.0), cval = np.min(data), u = 1.0)\n",
    "                    data = np.squeeze(data, axis = -1)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                # data = np.squeeze(data, axis = -1)\n",
    "                # plt.imshow(data, cmap = 'gray')\n",
    "                # plt.show()\n",
    "                # data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "                y_batch.append(labels_batch[i])\n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "            \n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "            \n",
    "            yield x_batch, y_batch\n",
    "\n",
    "def valid_generator(mode = '2D'):\n",
    "    while True:\n",
    "        for start in range(0, len(X_test), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            end = min(start + batch_size, len(X_test))\n",
    "            test_batch = X_test[start:end]\n",
    "            labels_batch = y_test[start:end]\n",
    "            \n",
    "            for i in range(len(test_batch)):\n",
    "                data, rate = librosa.load(test_batch[i], sr = SR)\n",
    "\n",
    "                if mode == '1D_psd':\n",
    "                    XX = np.zeros(129).astype('float32')\n",
    "                    XX = 10 * np.log10(signal.welch(data, fs = SR, window = 'hanning', nperseg = 256, noverlap = 128 + 64)[1])\n",
    "                    data = XX\n",
    "\n",
    "                if mode == '2D':\n",
    "                    data = librosa.stft(data, n_fft = N_FFT, hop_length = HOP_LEN)\n",
    "                    data = librosa.amplitude_to_db(np.abs(data))\n",
    "\n",
    "                    data = np.flipud(data)\n",
    "\n",
    "                data = np.expand_dims(data, axis = -1)\n",
    "\n",
    "                x_batch.append(data)\n",
    "                y_batch.append(labels_batch[i])\n",
    "\n",
    "            x_batch = np.array(x_batch, np.float32)\n",
    "            y_batch = np.array(y_batch, np.float32)\n",
    "            \n",
    "            y_batch = np_utils.to_categorical(y_batch, len(target_names))\n",
    "            \n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_mode != '2D':\n",
    "    model = basic_cnn_gru_1D(input_shape_1D[0], input_shape_1D[1], len(target_names))\n",
    "else:\n",
    "    model = basic_cnn_gru_2D(input_shape_2D[0], input_shape_2D[1], input_shape_2D[2], len(target_names))\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor = monitor,\n",
    "                                filepath = best_weights_path, \n",
    "                                save_best_only = True, \n",
    "                                save_weights_only = True,\n",
    "                                verbose = 1), \n",
    "                    EarlyStopping(monitor = monitor,\n",
    "                                patience = es_patience, \n",
    "                                verbose = 1),\n",
    "                    ReduceLROnPlateau(monitor = monitor,\n",
    "                                factor = 0.1, \n",
    "                                patience = rlr_patience, \n",
    "                                verbose = 1),\n",
    "                    CSVLogger(filename = log_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3022/3023 [============================>.] - ETA: 0s - loss: 0.6793 - acc: 0.7467\n",
      "Epoch 00001: val_acc improved from -inf to 0.82739, saving model to /home/kalfasyan/projects/wingbeat_frequencies/temp_data/cnn_gru.h5\n",
      "3023/3023 [==============================] - 1317s 436ms/step - loss: 0.6793 - acc: 0.7467 - val_loss: 0.4608 - val_acc: 0.8274\n",
      "Epoch 2/100\n",
      "3022/3023 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.8271\n",
      "Epoch 00002: val_acc improved from 0.82739 to 0.85626, saving model to /home/kalfasyan/projects/wingbeat_frequencies/temp_data/cnn_gru.h5\n",
      "3023/3023 [==============================] - 239s 79ms/step - loss: 0.4775 - acc: 0.8271 - val_loss: 0.3889 - val_acc: 0.8563\n",
      "Epoch 3/100\n",
      "1318/3023 [============>.................] - ETA: 1:57 - loss: 0.4221 - acc: 0.8458\n",
      "Epoch 00004: val_acc did not improve\n",
      "3023/3023 [==============================] - 238s 79ms/step - loss: 0.3769 - acc: 0.8645 - val_loss: 0.4673 - val_acc: 0.8198\n",
      "Epoch 5/100\n",
      "2234/3023 [=====================>........] - ETA: 54s - loss: 0.3548 - acc: 0.8721"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator(mode = model_mode),\n",
    "    steps_per_epoch = int(math.ceil(float(len(X_train)) / float(batch_size))),\n",
    "    validation_data = valid_generator(mode = model_mode),\n",
    "    validation_steps = int(math.ceil(float(len(X_test)) / float(batch_size))),\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks_list,\n",
    "    shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(best_weights_path)\n",
    "\n",
    "loss, acc = model.evaluate_generator(valid_generator(mode = model_mode),\n",
    "        steps = int(math.ceil(float(len(X_test)) / float(batch_size))))\n",
    "\n",
    "print('loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
